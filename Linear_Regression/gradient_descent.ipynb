{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bb15f1",
   "metadata": {},
   "source": [
    "Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2265111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78c171",
   "metadata": {},
   "source": [
    "Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcde852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_train = np.array([3.2, 5.1, 7.0, 9.1, 11.2, 12.9, 15.1, 16.8, 19.0, 21.1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa58a4",
   "metadata": {},
   "source": [
    "Define the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53e5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        total_cost += (f_wb - y[i]) ** 2\n",
    "    \n",
    "    total_cost /= 2*m\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6bd2a",
   "metadata": {},
   "source": [
    "Define the gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6024bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2640d9",
   "metadata": {},
   "source": [
    "Define the gradient descent functiont that will take all of these computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf9b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_algo(x,y,w_in, b_in, num_iters, alpha, cost_func, gradient_func):\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    j_history = []\n",
    "    p_history = []\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        dj_dw, dj_db = gradient_func(x,y,w,b)\n",
    "        \n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i < 100000:\n",
    "            j_history.append(cost_func(x,y,w,b))\n",
    "            p_history.append([w,b])\n",
    "        \n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"iterations: {i:4}, cost: {j_history[-1]: 0.3e}, dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}, w: {w: 0.5e}, b: {b: 0.5e}\")\n",
    "        \n",
    "    return w, b, j_history, p_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4977413",
   "metadata": {},
   "source": [
    "Initialize all necessary values and execute the GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20328d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:    0, cost:  8.263e+01, dj_dw: -8.263e+01, dj_db: -1.205e+01, w:  4.13150e+00, b:  6.02500e-01\n",
      "iterations: 10000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 20000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 30000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 40000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 50000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 60000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 70000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 80000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "iterations: 90000, cost:  5.976e-03, dj_dw: -3.313e-14, dj_db: -6.573e-15, w:  1.98242e+00, b:  1.14667e+00\n",
      "Final Values of w and b are:   1.9824 &   1.1467 \n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "\n",
    "num_iters = 100000\n",
    "alpha = 0.05\n",
    "\n",
    "w_final, b_final, j_history, p_history = gradient_descent_algo(x_train, y_train, w, b, num_iters, alpha, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final Values of w and b are: {w_final: 8.4f} & {b_final: 8.4f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
